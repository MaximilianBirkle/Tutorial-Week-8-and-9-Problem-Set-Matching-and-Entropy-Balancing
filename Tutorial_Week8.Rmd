---
title: "Tutorial Week 8 and 9: Matching and Entropy Balancing"
subtitle: "Problem Set - Propensity Scores, Matching, and Synthetic Control"
author:
  - Maximilian Birkle
  - Daniel Lehmann
  - Henry Lucas
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
    keep_tex: false
    latex_engine: xelatex
  github_document:
    toc: true
    html_preview: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 10, fig.height = 6)
```

# Load Packages

```{r load-packages, include=FALSE}
library(tidyverse)      
library(haven)          
library(MatchIt)        
library(cobalt)         
library(randomForest)   
library(lmtest)         
library(sandwich)       
library(knitr)          
library(kableExtra)     
library(broom)          
library(Synth)   
library(stargazer)
library(patchwork)
library(cobalt)
library(Synth)
```
\newpage

# Part I: Propensity Scores, Matching, and Robust Post-Matching Inference

## Background

**Research Question:** Do **United Nations interventions** help shorten the duration of civil wars?

Gilligan and Sergenti (2008) use matching methods to re-evaluate earlier findings suggesting that UN interventions *prolong* conflict. They argue that this conclusion stems from **selection bias** — the UN tends to intervene in the *worst* conflicts.

**Dataset:** `war_pre_snapshots.dta`

Each row represents a **conflict episode** observed before a potential UN intervention. Our goal is to estimate the causal effect of UN involvement (`UN`) on the length of the conflict (`t1 - t0`), while balancing on key pre-treatment covariates.

## Load Data

```{r load-data-part1}
# Read the UN intervention dataset
data_un <- read_dta("war_pre_snapshots.dta")
```

## Define Treatment and Covariates

```{r define-variables}
# Treatment variable: UN intervention (1 = Yes, 0 = No)
treat <- data_un$UN

# Outcome variable: conflict duration (t1 - t0)
outcome <- data_un$t - data_un$t0

# Covariates for propensity score model
covar <- c(
  "inter", "deaths", "couprev", "sos", "drugs", "t0",
  "ethfrac", "pop", "lmtnest", "milper",
  "eeurop", "lamerica", "asia", "ssafrica"
)

# Create covariate matrix
X <- data_un[, covar]

dta_new <- data.frame(
  treat = treat,
  outcome = outcome,
  X)
```

---

## Q0. First Check of the Data

### Tasks:

1. Why might UN interventions **not** be randomly assigned across conflicts?
2. Which of the listed variables are most likely to confound the relationship between `UN` and conflict duration? Run a quick logistic regression and check.

```{r q0-selection-bias, results='asis'}
# Logistic regression: UN intervention as function of covariates
logit_selection <- glm(treat ~ inter + deaths + couprev + sos + drugs + t0 +
                         ethfrac + pop + lmtnest + milper +
                         eeurop + lamerica + asia + ssafrica,
                       data = dta_new,
                       family = binomial(link = "logit"))

# Create stargazer table
stargazer(
  logit_selection,
  type = "latex",
  title = "Selection into UN Intervention: Logistic Regression",
  header = FALSE,
  dep.var.labels = "UN Intervention (1 = Yes)",
  covariate.labels = c(
    "Internationalized Conflict",
    "Battle Deaths",
    "Coup/Revolution",
    "Strategic Oil Supply",
    "Drug Activity",
    "Conflict Start Year",
    "Ethnic Fractionalization",
    "Log Population",
    "Log Mountainous Terrain",
    "Military Personnel per Capita",
    "Eastern Europe",
    "Latin America",
    "Asia",
    "Sub-Saharan Africa",
    "Constant"
  ),
  no.space = TRUE,
  omit.stat = c("aic", "ll"),
  notes = "Standard errors in parentheses.",
  notes.align = "l",
  digits = 3
)

```

**Discussion:**

**Why might UN interventions not be randomly assigned?** 

Cases where the UN intervenes are quite different from where they do not, which is why UN missions are not randomly assigned.

The decision to intervene depends on a number of factors, including the UN Security Council's selection process and whether the UN even pays attention to these cases.

Because the "treated" (intervention) and "untreated" (no intervention) cases are so different on so many variables, it becomes almost impossible to distinguish whether a causal effect is due to the treatment itself or some function of these other confounding variables. When you have differences that large between units, you face an extreme problem of constructing the counterfactual—that is, figuring out what would have happened had the UN not intervened.

**Which variables show strong associations with UN intervention?**

Looking at the results in table 1, we can see a high positive coefficient for **Drug Activity** (coef=3.238, p<0.01), meaning that the UN is more likely to intervene where there is drug-related activity present. This also makes intuitive sense, as interventions are fostered by international drug trafficking concerns. This is a prime example of why it is hard to compare regimes across regions that could potentially receive UN intervention. Since regimes, rebel groups, or terrorist groups in politically unstable systems often build their economic foundation on scarce resources (e.g., diamonds, oil, etc.) and also drugs, their different capabilities (e.g., geographic location, natural resources) offer different incentives for the UN to intervene, especially when their economic activities pose a global threat, as is the case with drugs.

Similarly, in countries with high per capita **military personnel** (coef=−1.123, p<0.01), countries with higher military capacity are less likely to receive UN intervention. This also makes intuitive sense, as weak military states would make it easier and safer to intervene, and they have more need for external support.

Furthermore, countries located in **Latin America** are less likely to receive UN intervention (coef=−3.775, P<0.05). This finding might be slightly confusing at first glance, although it might be explained by a couple of points. As many states in Latin America have faced high levels of political instability and also have a long history with drug trafficking, we might assume that they would be more likely to receive UN interventions. The absence of this finding might be due to the fact that Latin America has always been undisputedly located in the sphere of influence of the United States—historically articulated in the Monroe Doctrine (1823)—which has carried out its own "peacekeeping missions" across the continent, not based on decisions of the UN or other multilateral actors after the Cold War. Looking at recent developments around the coast of Venezuela, this dynamic still dominates today.

Overall, this regression table gives some preliminary hints that UN interventions are indeed not randomly assigned. It demonstrates clear selection bias: the UN systematically intervenes in cases with specific, observable characteristics (like high drug activity or weak militaries) and avoids others (like those in Latin America). While other factors like battle deaths or ethnic fractionalization don't show a significant effect in this model, the strong predictors identified here are exactly why it is extremely useful to use matching to create a valid comparison group.

---

## Q1. Estimating Propensity Scores

### Theoretical Background

Let $T_i \in \{0,1\}$ be the treatment indicator and $X_i = (X_{i1}, X_{i2}, \ldots, X_{ip})$ the vector of pre-treatment covariates.

The **propensity score** is defined as:

$$e(X_i) = P(T_i = 1 \mid X_i)$$

A **propensity score** takes the necessary covariates and estimates the a maximum likelihood model of the conditional probability of treatment (with fitted values that are bounded between 0 and 1), and uses the predicted values from the estimation to collapse those covariates into a single scalar. 


### Tasks:

1. Define the propensity score
2. Estimate $\hat{e}(X_i)$ in two ways:
   - **(a)** Logistic regression: $\text{logit}(e(X_i)) = X_i' \beta$
   - **(b)** Random forest classifier
3. Report mean, SD, and range of $\hat{e}(X_i)$ for treated and control
4. Create histogram/density plot by treatment status

```{r q1-propensity-logit}
# (a) Logistic regression propensity score

ps_logistic <- glm(treat ~ inter + deaths + couprev + sos + drugs + t0 +
                     ethfrac + pop + lmtnest + milper +
                     eeurop + lamerica + asia + ssafrica,
                   data = dta_new,
                   family = binomial(link = "logit"),
                   na.action = na.exclude)


dta_new$ps_logistic_probs <- predict(ps_logistic, 
                                     newdata = dta_new, 
                                     type = "response")
```

```{r q1-propensity-rf}

# (b) Random forest propensity score

set.seed(68159) #for reproducability

ps_random_forest <- randomForest(
  factor(treat) ~ inter + deaths + couprev + sos + drugs + t0 +
    ethfrac + pop + lmtnest + milper +
    eeurop + lamerica + asia + ssafrica,
  data = dta_new,
  ntree = 1000,
  importance = TRUE
)

# Extract predicted probabilities

dta_new$ps_random_forest <- predict(ps_random_forest, type = "prob")[, "1"]

```

```{r q1-summary-stats}
# Summary statistics of propensity scores

# Logistic Regression Stats
logit_control_mean <- mean(dta_new$ps_logistic_probs[dta_new$treat == 0], na.rm = TRUE)
logit_control_sd   <- sd(dta_new$ps_logistic_probs[dta_new$treat == 0], na.rm = TRUE)
logit_control_min  <- min(dta_new$ps_logistic_probs[dta_new$treat == 0], na.rm = TRUE)
logit_control_max  <- max(dta_new$ps_logistic_probs[dta_new$treat == 0], na.rm = TRUE)

logit_treated_mean <- mean(dta_new$ps_logistic_probs[dta_new$treat == 1], na.rm = TRUE)
logit_treated_sd   <- sd(dta_new$ps_logistic_probs[dta_new$treat == 1], na.rm = TRUE)
logit_treated_min  <- min(dta_new$ps_logistic_probs[dta_new$treat == 1], na.rm = TRUE)
logit_treated_max  <- max(dta_new$ps_logistic_probs[dta_new$treat == 1], na.rm = TRUE)

# Random Forest Stats
rf_control_mean <- mean(dta_new$ps_random_forest[dta_new$treat == 0], na.rm = TRUE)
rf_control_sd   <- sd(dta_new$ps_random_forest[dta_new$treat == 0], na.rm = TRUE)
rf_control_min  <- min(dta_new$ps_random_forest[dta_new$treat == 0], na.rm = TRUE)
rf_control_max  <- max(dta_new$ps_random_forest[dta_new$treat == 0], na.rm = TRUE)

rf_treated_mean <- mean(dta_new$ps_random_forest[dta_new$treat == 1], na.rm = TRUE)
rf_treated_sd   <- sd(dta_new$ps_random_forest[dta_new$treat == 1], na.rm = TRUE)
rf_treated_min  <- min(dta_new$ps_random_forest[dta_new$treat == 1], na.rm = TRUE)
rf_treated_max  <- max(dta_new$ps_random_forest[dta_new$treat == 1], na.rm = TRUE)


# Data Frame with results
summary_stats_q1 <- data.frame(
  Statistic = c("Mean", "Std. Deviation", "Min", "Max"),
  Logit_Control = c(logit_control_mean, logit_control_sd, logit_control_min, logit_control_max),
  Logit_Treated = c(logit_treated_mean, logit_treated_sd, logit_treated_min, logit_treated_max),
  RF_Control = c(rf_control_mean, rf_control_sd, rf_control_min, rf_control_max),
  RF_Treated = c(rf_treated_mean, rf_treated_sd, rf_treated_min, rf_treated_max)
)

tbl <- knitr::kable(
  summary_stats_q1, 
  digits = 3, 
  col.names = c("Statistic", "Control (T=0)", "Treated (T=1)", "Control (T=0)", "Treated (T=1)"),
  caption = "Table 1: Propensity Score Distribution by Model and Treatment Status"
)

if (knitr::is_latex_output() || knitr::is_html_output()) {
  tbl <- tbl %>%
    kableExtra::add_header_above(c(" " = 1, "Logistic Regression" = 2, "Random Forest" = 2)) %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
}

tbl

```

```{r q1-density-plot}
# Density plot of propensity scores by treatment status

# Logistic Regression Plot
p_logit <- ggplot(dta_new, aes(x = ps_logistic_probs, fill = factor(treat))) +
  geom_density(alpha = 0.7) +
  scale_fill_manual(
    name = "Treatment Status",
    values = c("0" = "blue", "1" = "red"),
    labels = c("0" = "Control (No UN)", "1" = "Treated (UN)")
  ) +
  labs(
    title = "A. Logistic Regression Scores",
    x = "Propensity Score",
    y = "Density"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") # Move legend to the bottom

# Random Forest Plot
p_rf <- ggplot(dta_new, aes(x = ps_random_forest, fill = factor(treat))) +
  geom_density(alpha = 0.7) +
  scale_fill_manual(
    name = "Treatment Status",
    values = c("0" = "blue", "1" = "red"),
    labels = c("0" = "Control (No UN)", "1" = "Treated (UN)")
  ) +
  labs(
    title = "B. Random Forest Scores",
    x = "Propensity Score",
    y = "Density"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom") # Move legend to the bottom

# --- 3. Combine the plots ---
p_logit + p_rf
```

**Interpretation:**
Looking at the propensity scores, it's pretty clear we have a major problem with common support, which is going to make the matching analysis tough. 

Our control group, all 1,211 of them, are just piled up at the low end of the distribution. For the logistic model, their p-scores have a tiny mean of 0.010 and don't go any higher than 0.194. This makes sense — the model correctly sees that these conflicts had an almost zero chance of getting a UN intervention.

In sharp contrast, our 16 treated units are all over the map. Their p-scores are spread wide, from 0.000 all the way to 0.884. This shows that, as we'd expect, the model found them to be much more likely to get an intervention.

The core problem is the total disconnect between these two groups. The "common support" — the region where we actually have both treated and control units — is stuck in that tiny window between 0.000 and 0.194. This means any treated unit with a p-score higher than 0.194 has literally no comparable units to match with.

The density plots show this perfectly. We see this massive, high-density blue spike at zero (the controls), while our small treated group is just a low, flat red line that's almost invisible. The plot's Y-axis has to stretch to fit the control group, which just visually hides the treated group and highlights how extreme this imbalance is.

This lack of overlap is definitely going to have direct implications for Q2. When we apply the matching algorithm, we have to expect that a large portion of our 16 treated units will fail to find a match and just get discarded. This isn't an error; it's a key finding that the conflicts the UN intervened in are, based on this data, just a fundamentally different beast from the ones it did not. 


---

## Q2. Implement 1:1 Nearest-Neighbor Matching

### Matching Setup

For each estimated propensity score $\hat{e}(X_i)$, match each treated unit to the nearest control on the **logit of the propensity score**:

$$\ell_i = \log\left( \frac{\hat{e}(X_i)}{1 - \hat{e}(X_i)} \right)$$

Use **replacement** and a **caliper** of $0.2 \times \text{SD}(\ell_i)$ to restrict poor matches.

### Tasks:

1. Implement matching using both logit and RF propensity scores
2. Report how many treated units fail to find a match
3. How does this change the estimand?

```{r q2-matching-logit-randomforest}

# Logit-Score Variables
dta_new$logit_ps_logit <- log(dta_new$ps_logistic_probs / (1 - dta_new$ps_logistic_probs))
dta_new$logit_ps_rf    <- log(dta_new$ps_random_forest / (1 - dta_new$ps_random_forest))

# We must remove p-scores of our RF model that 0 or 1, since they are unmatchable.
dta_new$logit_ps_logit[!is.finite(dta_new$logit_ps_logit)] <- NA
dta_new$logit_ps_rf[!is.finite(dta_new$logit_ps_rf)]       <- NA

# MatchIt requires a dataset with no NAs in the variables used.
dta_complete <- na.omit(dta_new)

# Match using Logistic Regression Scores
m_logit <- matchit(
  treat ~ 1,                        # Formula: treatment ~ no covariates
  data = dta_complete,
  distance = dta_complete$logit_ps_logit, # Use our pre-built logit-score
  method = "nearest",             # Use nearest neighbor
  ratio = 1,                        # 1:1 matching
  replace = TRUE,                   # Use replacement
  caliper = 0.2,                    # Caliper width
  std.caliper = TRUE                # TRUE = 0.2 * SD(distance)
)

# Match using Random Forest Scores
m_rf <- matchit(
  treat ~ 1,
  data = dta_complete,
  distance = dta_complete$logit_ps_rf, # Use our pre-built RF logit-score
  method = "nearest",
  ratio = 1,
  replace = TRUE,
  caliper = 0.2,
  std.caliper = TRUE
)


```

```{r q2-matching_summary, include=FALSE}

# Summary for Logit Match
summary(m_logit)

# Summary for Random Forest Match
summary(m_rf)
```

```{r q2-matching_table, results='asis'}

# Numbers from summary() - output
results_df <- data.frame(
  Status = c("All", "Matched", "Unmatched"),
  Logit_Control = c(139, 8, 131),
  Logit_Treated = c(15, 9, 6),
  RF_Control = c(139, 6, 133),
  RF_Treated = c(15, 8, 7)
)

# Professional Table for visualization
knitr::kable(results_df,
      format = "latex", 
      booktabs = TRUE, 
      col.names = c("Sample", "Control", "Treated", "Control", "Treated"),
      align = "lcccc",
      caption = "Sample Sizes Before and After 1:1 Matching"
) %>%
  add_header_above(c(" " = 1, "Logistic Regression" = 2, "Random Forest" = 2)) %>%
  kable_styling(latex_options = "hold_position", position = "center")
```


**Discussion:**

When we ran the matching procedure, we immediately ran into a major, real-world problem that confirms our earlier fears about poor overlap. We tried to find a "statistical twin" for each of our 15 UN-treated conflicts, using the standard 0.2 caliper on the logit of the p-score.

It turns out, this was only possible for about half of them. Using the logistic model's scores, we found acceptable matches for only 9 treated units. The random forest was even stricter, finding matches for only 8. This means that a huge chunk of our treated sample—roughly 45%—had to be discarded.

These 6 or 7 units weren't dropped by accident. They were dropped because they were just too "extreme" and fell completely outside the region of common support. The caliper, which is designed to prevent us from making bad, biased comparisons, correctly identified that these units had no reasonable counterfactual in the control group. We are almost certainly losing the most severe, high-profile conflicts, the ones that were so obviously candidates for UN intervention that they looked nothing like the non-intervened cases.

This has a crucial and unavoidable consequence for our research question. We've been forced to make a classic trade-off. We can no longer estimate the "Average Treatment Effect on the Treated" (ATT) for all conflicts the UN intervened in. That original goal is off the table because we had to discard half the data.

Instead, our estimand has fundamentally changed. We're now estimating the ATT for a much narrower, non-random subset of conflicts—specifically, the 8 or 9 "matchable" conflicts whose characteristics were already 'close enough' to the control group to begin with.

This is the central challenge of this kind of research. By discarding the 'unmatchable' conflicts, we've hopefully created an estimate that is internally valid for the remaining units. But we've paid a steep price in external validity, or generalizability. Our final causal conclusion, whatever it is, won't apply to all UN interventions. It will only apply to this small, specific group of conflicts that we were actually able to find a match for.

---

## Q3. Standardized Mean Differences (SMDs)

### SMD Formulas

For each covariate $X^k$:

**Before matching (ATT version):**

$$\text{SMD}_{\text{raw}}(k) = \frac{\bar{X}^k_{T=1} - \bar{X}^k_{T=0}}{\sqrt{s^{2,k}_{T=1}}}$$

**After matching:**

$$\text{SMD}_{\text{match}}(k) = \frac{\bar{X}^{k,\text{treated}}_{\text{match}} - \bar{X}^{k,\text{control}}_{\text{match}}}{\sqrt{s^{2,k}_{T=1}}}$$

### Tasks:

1. Compute SMDs before and after matching for all covariates
2. Create a Love plot showing balance before and after matching (both methods)
3. Add vertical line at 0.1 (acceptable threshold)
4. Comment on which design achieves better covariate balance
5. Create two additional Love plots including interactions and squared terms

```{r q3-smd, include=TRUE}

# Object 1: Raw (Unmatched) Data
bal_raw <- bal.tab(
  treat ~ inter + deaths + couprev + sos + drugs + t0 +
    ethfrac + pop + lmtnest + milper +
    eeurop + lamerica + asia + ssafrica,
  data = dta_complete, 
  estimand = "ATT" 
)

# Object 2: Logit-Matched Data
bal_logit <- bal.tab(m_logit)

# Object 3: RF-Matched Data
bal_rf <- bal.tab(m_rf)

```

```{r q3-balance-table, results='asis'}

# New dataframe by pulling from our objects
smd_df <- data.frame(
  Covariate = rownames(bal_raw$Balance),
  Unadjusted = bal_raw$Balance$Diff.Un,
  Logit_Matched = bal_logit$Balance$Diff.Adj,
  RF_Matched = bal_rf$Balance$Diff.Adj
)

# Professional Table for visualization
knitr::kable(smd_df,
      format = "latex", 
      booktabs = TRUE, 
      digits = 3,
      col.names = c("Covariate", "Unadjusted", "Logit Matched", "RF Matched"),
      align = "lrrr",
      caption = "Standardized Mean Differences Before and After Matching"
) %>%
  row_spec(nrow(smd_df)-1, hline_after = TRUE) %>% 
  kable_styling(latex_options = "hold_position", position = "center")
```


```{r q3-love-plot-base}

# Firstly, we have to define the base formula with the base covariates
formula_base <- ~ inter + deaths + couprev + sos + drugs + t0 +
                  ethfrac + pop + lmtnest + milper +
                  eeurop + lamerica + asia + ssafrica

# We create three balance objects
bal_raw <- bal.tab(
  formula_base, 
  data = dta_complete, 
  treat = dta_complete$treat,
  estimand = "ATT"
)

bal_logit <- bal.tab(
  formula_base,
  data = dta_complete,
  treat = dta_complete$treat,
  weights = m_logit$weights, # Use direct weights
  estimand = "ATT",
  method = "weighting"
)

bal_rf <- bal.tab(
  formula_base,
  data = dta_complete,
  treat = dta_complete$treat,
  weights = m_rf$weights,
  estimand = "ATT",
  method = "weighting"
)

# SMD data frame
smd_df <- data.frame(
  Covariate = rownames(bal_raw$Balance),
  Unadjusted = bal_raw$Balance$Diff.Un,
  Logit_Matched = bal_logit$Balance$Diff.Adj,
  RF_Matched = bal_rf$Balance$Diff.Adj
)

# Reshape and order
covariate_order <- smd_df %>%
  mutate(Absolute_SMD = abs(Unadjusted)) %>%
  arrange(desc(Absolute_SMD)) %>%
  pull(Covariate)

smd_long_ordered <- smd_df %>%
  pivot_longer(cols = -Covariate, names_to = "Sample", values_to = "SMD") %>%
  mutate(
    Absolute_SMD = abs(SMD),
    Covariate = factor(Covariate, levels = covariate_order)
  )

# Create the plot
ggplot(smd_long_ordered, aes(x = Absolute_SMD, y = Covariate, color = Sample, shape = Sample)) +
  geom_point(size = 3, position = position_dodge(width = 0.6)) + 
  geom_vline(xintercept = 0.1, linetype = "dashed", color = "black") +
  labs(
    title = "Covariate Balance (Base)",
    x = "Absolute Standardized Mean Difference",
    y = "Covariate"
  ) +
  scale_color_manual(
    name = "Sample",
    values = c("Unadjusted" = "grey30", "Logit_Matched" = "blue", "RF_Matched" = "red")
  ) +
  scale_shape_manual(
    name = "Sample",
    values = c("Unadjusted" = 15, "Logit_Matched" = 17, "RF_Matched" = 18)
  ) +
  theme_bw() + 
  theme(legend.position = "bottom") + 
  coord_cartesian(xlim = c(0, NA))

```

```{r q3-love-plot-interactions}
# Love plot: Including interactions

# Firstly, we have to define the formula with the interaction terms
formula_interactions <- ~ inter + deaths + couprev + sos + drugs + t0 +
                            ethfrac + pop + lmtnest + milper +
                            eeurop + lamerica + asia + ssafrica +
                            inter*pop + inter*milper + pop*milper

# Again, we create three balance objects
bal_raw_int <- bal.tab(
  formula_interactions, 
  data = dta_complete, 
  treat = dta_complete$treat,
  estimand = "ATT"
)

bal_logit_int <- bal.tab(
  formula_interactions,
  data = dta_complete,
  treat = dta_complete$treat,
  weights = m_logit$weights,
  estimand = "ATT",
  method = "weighting"
)

bal_rf_int <- bal.tab(
  formula_interactions,
  data = dta_complete,
  treat = dta_complete$treat,
  weights = m_rf$weights,
  estimand = "ATT",
  method = "weighting"
)

# SMD data frame (with interactions)
smd_df_int <- data.frame(
  Covariate = rownames(bal_raw_int$Balance),
  Unadjusted = bal_raw_int$Balance$Diff.Un,
  Logit_Matched = bal_logit_int$Balance$Diff.Adj,
  RF_Matched = bal_rf_int$Balance$Diff.Adj
)

# Bringing our data into shape and reorder it
covariate_order_int <- smd_df_int %>%
  mutate(Absolute_SMD = abs(Unadjusted)) %>%
  arrange(desc(Absolute_SMD)) %>%
  pull(Covariate)

smd_long_int_ordered <- smd_df_int %>%
  pivot_longer(cols = -Covariate, names_to = "Sample", values_to = "SMD") %>%
  mutate(
    Absolute_SMD = abs(SMD),
    Covariate = factor(Covariate, levels = covariate_order_int)
  )

# Create plot with interactions
ggplot(smd_long_int_ordered, aes(x = Absolute_SMD, y = Covariate, color = Sample, shape = Sample)) +
  geom_point(size = 3, position = position_dodge(width = 0.6)) + 
  geom_vline(xintercept = 0.1, linetype = "dashed", color = "black") +
  labs(
    title = "Covariate Balance (with Key Interaction Terms)",
    x = "Absolute Standardized Mean Difference",
    y = "Covariate"
  ) +
  scale_color_manual(
    name = "Sample",
    values = c("Unadjusted" = "grey30", "Logit_Matched" = "blue", "RF_Matched" = "red")
  ) +
  scale_shape_manual(
    name = "Sample",
    values = c("Unadjusted" = 15, "Logit_Matched" = 17, "RF_Matched" = 18)
  ) +
  theme_bw() + 
  theme(legend.position = "bottom") + 
  coord_cartesian(xlim = c(0, NA))

```

```{r q3-love-plot-squares}
# Love plot: Including squared terms

# Firstly, we have to define the formula with squared terms
formula_squares <- ~ inter + deaths + couprev + sos + drugs + t0 +
                     ethfrac + pop + lmtnest + milper +
                     eeurop + lamerica + asia + ssafrica +
                     I(deaths^2) + I(t0^2) + I(ethfrac^2) + 
                     I(pop^2) + I(lmtnest^2) + I(milper^2)

# We create three balance objects
bal_raw_poly <- bal.tab(
  formula_squares, 
  data = dta_complete, 
  treat = dta_complete$treat,
  estimand = "ATT"
)

bal_logit_poly <- bal.tab(
  formula_squares,
  data = dta_complete,
  treat = dta_complete$treat,
  weights = m_logit$weights,
  estimand = "ATT",
  method = "weighting"
)

bal_rf_poly <- bal.tab(
  formula_squares,
  data = dta_complete,
  treat = dta_complete$treat,
  weights = m_rf$weights,
  estimand = "ATT",
  method = "weighting"
)

# SMD data frame (for squared terms)
smd_df_poly <- data.frame(
  Covariate = rownames(bal_raw_poly$Balance),
  Unadjusted = bal_raw_poly$Balance$Diff.Un,
  Logit_Matched = bal_logit_poly$Balance$Diff.Adj,
  RF_Matched = bal_rf_poly$Balance$Diff.Adj
)


covariate_order_poly <- smd_df_poly %>%
  mutate(Absolute_SMD = abs(Unadjusted)) %>%
  arrange(desc(Absolute_SMD)) %>%
  pull(Covariate)

smd_long_poly_ordered <- smd_df_poly %>%
  pivot_longer(cols = -Covariate, names_to = "Sample", values_to = "SMD") %>%
  mutate(
    Absolute_SMD = abs(SMD),
    Covariate = factor(Covariate, levels = covariate_order_poly)
  )

# Create the plot (squared terms)
ggplot(smd_long_poly_ordered, aes(x = Absolute_SMD, y = Covariate, color = Sample, shape = Sample)) +
  geom_point(size = 3, position = position_dodge(width = 0.6)) + 
  geom_vline(xintercept = 0.1, linetype = "dashed", color = "black") +
  labs(
    title = "Covariate Balance (with Key Squared Terms)",
    x = "Absolute Standardized Mean Difference",
    y = "Covariate"
  ) +
  scale_color_manual(
    name = "Sample",
    values = c("Unadjusted" = "grey30", "Logit_Matched" = "blue", "RF_Matched" = "red")
  ) +
  scale_shape_manual(
    name = "Sample",
    values = c("Unadjusted" = 15, "Logit_Matched" = 17, "RF_Matched" = 18)
  ) +
  theme_bw() + 
  theme(legend.position = "bottom") + 
  coord_cartesian(xlim = c(0, NA))


```

**Interpretation:**

The "Covariate Balance (Base)" plot is our main report card, and it's not good. The unadjusted data (grey squares) clearly shows the mess we started with. Key variables like pop, milper, and lmtnest are way off the 0.1 mark, which is exactly the selection bias we're trying to fix. But after we ran our matching, both methods failed to solve the problem. The logistic model (blue triangles) still left variables like t0 and inter way out of balance. The random forest (red diamonds) was a total disaster—it actually worsened the imbalance for critical covariates like lmtnest and t0, pushing their SMDs to new highs of 0.6 or more.

This failure just gets confirmed and amplified in the other two plots. The "Key Squared Terms" plot shows that, unsurprisingly, since we couldn't even balance pop and milper, we definitely didn't balance pop^2 or milper^2. And the "Key Interaction Terms" plot is the most conclusive. The unadjusted data had astronomical SMDs for some interactions (like inter_0 * milper at over 1.2), and both matching strategies were completely ineffective at fixing this. The "matched" groups still had SMDs floating around 1.0.

In summary, these Q3 diagnostics aren't a success story—they're a critical finding that connects everything we've seen so far. It all goes back to Q1 (the terrible overlap) and Q2 (having to discard half our treated units). These plots are the final proof that the 8 or 9 "matched" units we kept still aren't a valid comparison group. We have to conclude that this matching strategy failed. Any causal estimate we'd get from this sample would be unreliable and biased.

---

## Q4. Overlap

### Tasks:

1. For each method, report:
   - Min and max of $\hat{e}(X_i)$ for treated and controls
   - Proportion of treated units whose $\hat{e}(X_i)$ lies inside the support of controls (and vice versa)
2. Plot distributions of $\hat{e}(X_i)$ for treated and controls
3. Identify regions of poor overlap or extreme propensities
4. (Optional) Trim observations outside common support and re-compute ATT
5. Examine matched subsets - do matches seem like fair counterfactuals?

**Interpretation Q4.1**

To kick off our overlap diagnostics, we first just looked at the simple min/max range of our propensity scores from both models.
At first glance, the numbers tell a really clear story. For our 139 control units (the "No UN" group), the p-scores are all jammed at the low end. The logistic model gave them a max score of just 0.194, and the random forest was even tighter at 0.188. This just confirms the model is doing its job and thinks these conflicts had almost no chance of getting an intervention.
In sharp contrast, our 15 treated units are all over the place. The logistic model's scores go all the way up to 0.884, and the random forest goes up to 0.504.
What this tells us, before we even get to the next step, is that our common support is paper-thin. We have treated units with p-scores of 0.8, 0.6, and 0.5, but our entire pool of controls stops at 0.19. We have no one to match these high-score treated units to. This simple range check is the first big red flag that proves our overlap is terrible.

```{r q4-overlap-proportion}

# For the LOGIT Model

# Get the min/max p-score of the CONTROL group (the "common support")
control_support_logit <- range(dta_complete$ps_logistic_probs[dta_complete$treat == 0], na.rm = TRUE)

# Count how many TREATED units fall INSIDE that range
treated_on_support_logit <- sum(
  dta_complete$ps_logistic_probs[dta_complete$treat == 1] >= control_support_logit[1] &
  dta_complete$ps_logistic_probs[dta_complete$treat == 1] <= control_support_logit[2],
  na.rm = TRUE
)

# Get total treated units (should be 15 from dta_complete)
total_treated <- sum(dta_complete$treat == 1, na.rm = TRUE)

# Calculate the proportion
prop_logit <- (treated_on_support_logit / total_treated) * 100

# --- 2. For the RANDOM FOREST Model ---

# Get the min/max p-score of the CONTROL group
control_support_rf <- range(dta_complete$ps_random_forest[dta_complete$treat == 0], na.rm = TRUE)

# Count how many TREATED units fall INSIDE that range
treated_on_support_rf <- sum(
  dta_complete$ps_random_forest[dta_complete$treat == 1] >= control_support_rf[1] &
  dta_complete$ps_random_forest[dta_complete$treat == 1] <= control_support_rf[2],
  na.rm = TRUE
)

# Calculate the proportion
prop_rf <- (treated_on_support_rf / total_treated) * 100

# --- 3. Print the findings ---
cat(sprintf("Logit Model: %d out of %d treated units (%.1f%%) were on the common support.\n",
            treated_on_support_logit, total_treated, prop_logit))
            
cat(sprintf("Random Forest Model: %d out of %d treated units (%.1f%%) were on the common support.\n",
            treated_on_support_rf, total_treated, prop_rf))
```

This calculation gives us the hard numbers behind our overlap problem, and it reveals an interesting wrinkle.
For the logit model, our calculation shows that only 8 of our 15 treated units (53.3%) had a p-score that fell strictly within the common support region (0.000 to 0.194) defined by the controls.
This is a critical finding. In Q2, our caliper matching, which is a "fuzzier" search, was able to find 9 matches. This calculation, which uses a "hard" rule, finds only 8. This suggests that the 9th match we found was likely a very poor one, sitting right on the edge of the common support and barely squeaking by the caliper.
The story for the random forest is even more stark. Our calculation shows that only 5 of the 15 units (33.3%) were on the hard common support. This confirms our earlier findings that the RF model created worse overlap. The fact that our Q2 matching algorithm managed to find 8 matches suggests it had to pull in 3 units that were technically "off-support," leading to the severe imbalance we saw in the Q3 Love plots.
This step proves that our "unmatched" units weren't just a random assortment; they were all the units that fell outside the common support.

**Interpretation Q4.2**

The density plots we ran back in Q1 really drive home the findings from our min/max table.
When you look at those plots, you see this massive, high-density blue spike piled up right at zero. That's our 139 control units, all stacked together. In comparison, our 15 treated units are just a low, flat red line that's almost invisible. The Y-axis has to stretch so much to fit that giant blue spike that it visually squashes the red distribution, which perfectly illustrates the extreme imbalance we're dealing with.
The "region of poor overlap" is, to be specific, any p-score above 0.194. That's the absolute maximum score we found in the entire control group. Our min/max table showed that our treated units have p-scores going all the way up to 0.884. This means any treated unit with a score of 0.50, or even 0.25, is living in a "neighborhood" where there are literally zero control units. This makes finding a valid match for them impossible.

**Interpretation Q4.4**

The prompt asks for a "common sense" check to see if our matched pairs look "acceptable" or "fair." We could go through them one by one, but honestly, we've already got a much better, data-driven answer from our diagnostic checks in Q3.
Our Love plots give us the definitive answer, which is: no, the matched subsets are not acceptable.
Our main Covariate Balance (Base) plot, for instance, makes it crystal clear that even after matching, the 8-9 pairs we kept are still severely unbalanced. Key variables like t0, inter, and pop have SMDs way over the 0.1 line, and some—like t0 for the RF model—actually got worse after matching.
This just proves these matches aren't "fair" counterfactuals; we're still comparing apples and oranges. So, our entire Q3 analysis, which already concluded that the matching failed to get us balance, is the definitive answer to this question.

---

## Q5. Matched-Pair ATT

### ATT Estimator

Let each matched pair be denoted by $(i, j(i))$ where $i$ is treated and $j(i)$ is its matched control.

The **average treatment effect on the treated** is:

$$\widehat{\tau}_{\text{ATT}} = \frac{1}{N_T^*} \sum_{i \in \mathcal{T}^*} \left( Y_i - Y_{j(i)} \right)$$

where $\mathcal{T}^*$ is the set of treated units with a valid match.

### Task:

Compute the ATT for both matching methods.

```{r q5-att-calculation}

# ATT using LOGIT matching

# Get the matrix of matched pairs from the matchit object
match_matrix_logit <- m_logit$match.matrix

# Get the outcomes for the matched TREATED units (Yi)
y_i_logit <- dta_complete[rownames(match_matrix_logit), "outcome"]

# Get the outcomes for the matched CONTROL units (Yj(i))
y_j_logit <- dta_complete[match_matrix_logit[, 1], "outcome"]

# Calculate the difference for each pair
diffs_logit <- y_i_logit - y_j_logit

# Compute the ATT (the mean of the differences for valid pairs)
att_logit <- mean(diffs_logit, na.rm = TRUE)

cat("ATT (Logit Matched):", att_logit, "\n")


# ATT using RF matching

# Get the matrix of matched pairs
match_matrix_rf <- m_rf$match.matrix

# Get outcomes for TREATED units (Yi)
y_i_rf <- dta_complete[rownames(match_matrix_rf), "outcome"]

# Get outcomes for CONTROL units (Yj(i))
y_j_rf <- dta_complete[match_matrix_rf[, 1], "outcome"]

# Calculate the difference for each pair
diffs_rf <- y_i_rf - y_j_rf

# Compute the ATT (the mean of the differences for valid pairs)
att_rf <- mean(diffs_rf, na.rm = TRUE)

cat("ATT (RF Matched):", att_rf, "\n")


```

**Interpretation:**

The logit model, with its 9 matched pairs, spit out an ATT of -6.22. On the surface, this looks like it suggests the UN decreases conflict duration by about 6.2 (days/months, whatever the unit is).
But then, in sharp contrast, the random forest's 8 pairs gave us an ATT of +38.5, suggesting the UN prolongs conflict.
However, we have to be extremely clear: neither of these numbers is a credible causal estimate.
Our diagnostic analysis in Q3 was a definitive finding that our matching procedure failed. Those Love plots showed conclusively that our "matched" samples are still severely imbalanced on key, powerful confounders like the start year (t0) and whether the conflict was internationalized (inter).
Because our matched sample isn't balanced, the original selection bias we were trying to fix is still baked into the cake. This means these ATT estimates are hopelessly contaminated by that same bias.
The fact that the two methods give wildly different results—one negative, one positive—is basically the final nail in the coffin. It just confirms that our estimates are just noise, driven by the remaining imbalance.
Therefore, we can't interpret -6.22 or 38.5 as "the causal effect." We have to conclude that this entire matching strategy failed to produce a valid estimate. Calculating a simple mean-difference on this still-biased sample is just as uninterpretable as the raw, unadjusted comparison we started with.

---

## Q5.5. Bias–Variance Tradeoff in Matching Ratios

### (a) Conceptual Question

For 1-to-$m$ nearest-neighbor matching without replacement, the ATT estimator is:

$$\widehat{\tau}_{\text{ATT}}^{(m)} = \frac{1}{N_T^*} \sum_{i \in \mathcal{T}^*} \left( Y_i - \frac{1}{m}\sum_{j \in \mathcal{J}(i)} Y_j \right)$$

where $\mathcal{J}(i)$ is the set of the $m$ closest control matches for treated unit $i$.

**Tasks:**

1. Explain why increasing $m$ tends to:
   - **Decrease variance**
   - **Increase bias**
2. Discuss how this relates to distance in covariate space
3. If overlap is weak, which risk dominates as $m$ grows?

**Discussion:**

[Your explanation of the bias-variance tradeoff here]

---

### (b) Practical Exercise

**Tasks:**

1. Re-run matching for 1:1, 2:1, and 3:1 ratios (with replacement and same caliper)
2. Record: number matched, mean distance, ATT estimate
3. Compute cluster-robust standard errors for each design
4. Create results table
5. Plot ATT vs. $m$ with ±1.96 SE error bars

```{r q5-5-matching-ratios}
# 1:1 matching


# 2:1 matching


# 3:1 matching


```

```{r q5-5-results-table}
# Create comparison table


```

```{r q5-5-plot-ratios}
# Plot ATT by matching ratio with error bars


```

**Interpretation:**

[Do results display expected bias-variance pattern?]

---

### (c) Discussion

**Tasks:**

- Which design (1:1, 2:1, or 3:1) is most appropriate?
- How does observed pattern relate to Abadie & Imbens (2006)?
- What would happen with infinite data and perfect overlap?

**Discussion:**

[Your analysis here]

---

## Q6. Robust Post-Matching Inference (Abadie & Spiess, 2021)

### Regression with Cluster-Robust Standard Errors

After matching, fit the regression:

$$Y_i = \alpha + \tau T_i + \varepsilon_i$$

using only matched data.

Let $s(i)$ denote the **subclass (pair id)** of observation $i$.

Compute **cluster-robust standard errors** for $\hat{\tau}$ by clustering on $s(i)$:

$$\widehat{V}_{\text{CR}}(\hat{\tau}) = (X'X)^{-1} \left( \sum_{s} X_s' \hat{\varepsilon}_s \hat{\varepsilon}_s' X_s \right) (X'X)^{-1}$$

### Tasks:

1. Report $\hat{\tau}$ and its cluster-robust standard error
2. Compare results for logit-matched and RF-matched samples

```{r q6-robust-inference-logit}
# Regression on logit-matched data with cluster-robust SE


```

```{r q6-robust-inference-rf}
# Regression on RF-matched data with cluster-robust SE


```

```{r q6-comparison-table}
# Compare results


```

**Interpretation:**

[Compare point estimates and standard errors across methods]

---

## Q7. (Optional) Bootstrap Check

### Matched-Pair Bootstrap

**Warning:** Bootstraps are not theoretically valid for matching estimators, but this serves as a check.

**Tasks:**

1. Resample matched pairs (subclasses) with replacement
2. Recompute $\widehat{\tau}^{(b)}$ for each bootstrap sample $b = 1,\ldots,B$
3. Report bootstrap mean, SD, and percentile 95% CI
4. Compare to cluster-robust results

```{r q7-bootstrap-optional}
# Bootstrap procedure


```

**Discussion:**

[Do bootstrap and cluster-robust results tell a similar story?]

---

## Q8. Reflection

### Tasks:

1. Why does the propensity score $e(X_i)$ act as a **balancing score**?
2. How does random-forest estimation of $e(X_i)$ change matching results compared to logistic regression?
3. Why is overlap ($0 < e(X_i) < 1$) necessary for identifying the ATT?

**Discussion:**

[Your reflection here]

\newpage

# Part II: Synthetic Control - German Reunification Study

## Background

In 1990, West Germany underwent reunification with East Germany. The question: *What was the economic cost (or benefit) of this event on West Germany's GDP per capita?*

Using the synthetic control method, we construct a counterfactual "synthetic West Germany" from a weighted combination of other OECD countries.

**Paper:** Abadie, Diamond & Hainmueller (2015), *Comparative Politics and the Synthetic Control Method*, AJPS.

**Dataset:** Available via Harvard Dataverse (doi:10.7910/DVN/24714)

## Load Data

```{r load-data-part2}
# Read German reunification dataset
# Load the replication dataset
load("repgermany.RData")
repgermany <- x

# Display structure
#str(repgermany)
#head(repgermany)
```

---

## (a) Conceptual Questions

### Tasks:

1. Explain the intuition behind the synthetic control method. What kind of assignment problem does it address?
2. Why is it particularly suitable for the West Germany case?
3. What is the key identification assumption?

**Discussion:**

**1.)**

The synthetic control method serves as a way to bridge the quantitative/qualitative divide in comparative politics and possibly all of social science research, helping to make precise quantitative inference in small-sample studies. Usually, small sample comparative case studies uncover evidence at a level of high granularity that is impossible to establish in large-scale quantitative studies, which in turn provide precise numerical results that can be compared across studies. For these reasons calls became loud to combine the best of both worlds.

The goal of most study designs is to compare outcomes between treated units, that are the main objects of study, and similar but unaffected control units. The comparison units are intended to reproduce the counterfactual of the treated units in the absence of the treatment. When we try to make quantitative causal inference in small sample comparative studies, we usually do not fail due to the small sample size itself, but due to the missing mechanism of how to find suitable comparison units for the small pool of units we have.

Especially in comparative political science, we often look at aggregate entities, such as states, countries or regions, for which obvious single comparisons often do not exist. In that case we often have one aggregate unit receiving treatment, where we try to select a set of similar untreated units from all of the untreated cases ("donor pool") to approximate the counterfactual. Standard designs based on selection on observables try to remove bias by controlling for measurable covariates. This approach fails when unobserved factors confound the result. This is particularly prevalent in comparative political research, as we study large entities as treatment units and control units, being made up of many individuals.

However, the synthetic control method is based on the premise that when the units of analysis are a few aggregate entities, a combination of comparison units (the synthetic control) often does a better job of reproducing the characteristics of the unit or units representing the case of interest than any single comparison unit alone. The comparison unit in the synthetic control method is selected as the weighted average of all potential comparison units. This weighted average is optimized to best reproduce the treated unit's pre-treatment outcome path and other key predictors. The optimally-weighted average is what serves as the counterfactual, which is custom-built to be the treated unit's 'doppelgänger' before treatment occurred. The core intuition is that by replicating the pre-treatment trend, the synthetic control also replicates the unobserved factors that drove that trend, thus creating a valid counterfactual.

**2.)**

The case of West Germany is particularly suitable because no single country could serve as a good counterfactual comparison on its own. As an aggregate entity (a country), single comparisons often do not exist in comparative political science. The authors demonstrated that no single country from the donor pool (16 OECD countries) could closely approximate the values of pre-reunification economic growth predictors for West Germany. This lack of a good single match perfectly aligns with the synthetic control method's motivation, namely to construct a composite counterfactual from a weighted average of the donor pool. This weighted average (a combination of a few OECD countries) provided a very close approximation to West Germany's economic attributes and pre-treatment outcome path prior to 1990.

**3.)**

The key identification assumption of the synthetic control method is rooted in the belief that similar pre-treatment trajectories imply similar causal structures. The authors assume that if the unit representing the case of interest and the synthetic control exhibit similar outcomes over an extended period of time prior to the intervention, then any discrepancy in the outcome variable following the intervention is interpreted as a product of the intervention itself. The ultimate goal is for the synthetic control to accurately reproduce the unobserved counterfactual outcome path that the treated unit (West Germany) would have experienced in the absence of the treatment (reunification). Therefore, the method aims to find a weighted average of multiple control units that collectively replicates the entire pre-treatment trajectory of the outcome variable ($Y$ over $t$), in addition to matching on key predictor variables ($X$).

---

## (b) Mathematical/Optimization Questions

### The Optimization Problem

The synthetic control method solves:

$$\min_{w} \sum_{t \le T_0} \left(Y_{1t} - \sum_{j=2}^{J+1} w_j Y_{jt}\right)^2$$

subject to:

$$w_j \ge 0, \quad \sum_j w_j = 1$$

### Tasks:

1. Write and explain each term in the optimization problem
2. What role do $v$-weights play in predictor balancing?
3. Why is the convex-combination constraint important? What if weights could be negative or sum $\neq$ 1?

**Mathematical Discussion:**

The optimization problem in synthetic control is the problem to find the ideal set of weights to build the most similar looking control group to the treatment group pre-treatment. To answer the above questions in more detail, we first have to define the meaning of the underlying terminology.

**The Synthetic Control**

First of all, we have a sample of $J+1$ units of analysis (e.g. countries), where $j=1$ is the case of interest ("treated unit"), and the units ($j=2$ to $j=J$) define the "Donor pool" of potential comparison units. The goal of the Synthetic control is to find a weighted average of the donor pool, that is a $(J \times 1)$ vector of weights $W= (w_2, ..., w_{J+1})',$ with $0 \leq w_j \leq 1$ for $j=2, ..., J$ and $w_2+...+w_{J+1}=1$, that the characteristics of the treated unit is best resembled by the synthetic control. 
Let $X_1$ be a $(k\times 1)$ vector of preintervention characteristics for the treated unit and $X_0$ be a $(k\times J)$ matrix of the same variables for the donor pool ($j=2$ to $j=J$). The optimization problem is defined as to find a $W^*$ that minimizes $\|X_1-X_0W\|$, the difference between the treated unit's characteristics and the synthetic control. 

**Operationalization**
Given $m=1, ...,k$, $X_{1m}$ is the value of the $m$-th variable for the treated unit and $X_{0m}:$ is a $(1 \times J)$ vector containing the values of the $m$-th variable for the units in the donor pool. $W^*$ is chosen to minimize:

$\sum_{m=1}^{k}v_m(X_{1m}-X_{0m}W)^2$


**Regarding Question 2.)**

$v_m$ is a wight that reflects the relative importance assigned to each of the predictor variables, when the difference between $X_1 \text{ and } X_0W$ is measured. It is crucial that the synthetic control closely reproduces the values that variables with a large predictive power on the outcome of interest take for the unit affected by the intervention. Therefore, variables with high predictive power, logically receive high $v$-weights.

The $v$-weights therefore scale the influence of a potential mismatch between the pre-treatment case of interest and the synthetic control. The unit weights $w_j$ however, determine the contribution of each single unit $(j=2 \text{ to } j=J)$ in our data (or donor pool), to the overall value of that value in the synthetic control condition. Therefore, the $v$-weights, scale the relative importance of the $w_j$ weights for a variable $m$. Selecting a high value for $v$, also leads to a stronger influence of an individual units ($j$) difference between treatment and synthetic control. Where variables are assumed to receive a higher weight, values of $w_j$ should also be chosen more carefully. Selecting low values for $v$, leads to lower influence of difference in pre-treatment characteristics of the treated and the synthetic control.


**The Synthetic Control Estimator**

Let $Y_{it}$ denote the outcome of unit $j$ at time $t$, $Y_1$ be a $(T_1 \times 1)$ vector collecting the post-intervention values of the outcome for the treated unit, and $Y_0$ be a $(T_1 \times J)$ matrix where column $j$ contains the post-intervention values of the outcome for unit $j+1$. For a post-intervention period $t$ (with $t \geq T_0$), the synthetic control estimator of the effect of the treatment is given by the difference between the outcome of the treated unit $(Y_{1t})$ and the outcome for the synthetic control at that period, which equals the difference between the post-intervention outcomes of the treated unit and the synthetic control $Y_1-Y_0W^*$:

$$Y_{1t}- \sum_{j=2}^{J+1} w_j^* Y_{jt}$$

**Regarding Question 3:**

Recalling the premise and the motivation of the synthetic control method, the goal is to construct a combination of comparison units to reproduce the characteristics of the unit or units representing the case of interest. By definition, the comparison units are constructed by taking the weighted average of all potential comparison units that best resemble the characteristics of the case of interest. 

Therefore we can answer the question of what would happen if the weights "could" be negative or sum up to different than one, using just some basic statistics definitions. A mathematical average (or mean) is the sum of all numbers in a set divided by the count of how many numbers are in that set. A weighted average is the sum of a set of numbers where each number has first been multiplied by its own "importance" or "weight" (as explained in the paper). When the weights are negative or greater than one, it is no longer a weighted average, but extrapolation, where a weight greater than one helps to project beyond, and a negative weight means to subtract that unit's contribution, creating a value that has a negative effect. 

According to the authors, this is exactly what we see in regression-based approaches. Although they use synthetic comparison units having coefficients that sum up to one as well, the regression approach **does not** restrict the coefficients of the linear combination that define the comparison unit to be between zero and one. If we now assume that our individual weights $(w_j)$ in the synthetic control term could also have values beyond $[0, 1]$, this would mean that we would just have a simple linear regression model, whose weights extrapolate to produce a perfect fit, in order to minimize the error. The findings of the paper show, however, that extrapolation is not necessary in the context of this study, since there exists a synthetic control that closely fits the values of the characteristics of the units and that does not extrapolate outside of the support of the data.

---

## (c) Estimation, Balance Before & After

### Tasks:

1. Estimate synthetic control for West Germany over pre-treatment period
2. Compute balance table of key predictors (GDP, trade openness, inflation, schooling, investment) showing treated vs. synthetic mean **before treatment**
3. Report non-zero weights $w_j$
4. Interpret: which donor countries dominate and why?
5. Assess whether pre-treatment fit is acceptable for credible inference

```{r synth-estimation, message=FALSE, warning=FALSE}
# Prepare data and identify West Germany's index
west_germany_id <- repgermany %>% 
  filter(country == "West Germany") %>% 
  pull(index) %>% 
  unique()

# Synthetic control estimation (silently)
invisible(capture.output({
  dataprep_out <- dataprep(
    foo = as.data.frame(repgermany),
    predictors = c("gdp", "trade", "infrate", "industry"),
    predictors.op = "mean",
    time.predictors.prior = 1981:1990,
    special.predictors = list(
      list("invest80", 1980:1985, "mean"),
      list("schooling", 1980:1985, "mean")
    ),
    dependent = "gdp",
    unit.variable = "index",
    unit.names.variable = "country",
    time.variable = "year",
    treatment.identifier = west_germany_id,
    controls.identifier = setdiff(unique(repgermany$index), west_germany_id),
    time.optimize.ssr = 1960:1989,
    time.plot = 1960:2003
  )
  
  # Optimization
  synth_out <- synth(data.prep.obj = dataprep_out, method = "BFGS")
}))
```

```{r synth-balance-table}
# Create balance table for pre-treatment predictors
balance_table <- synth.tab(
  dataprep.res = dataprep_out,
  synth.res = synth_out
)

# Create a formatted balance table
balance_df <- data.frame(
  Predictor = c("GDP per capita", "Trade openness", "Inflation rate", 
                "Industry share", "Schooling", "Investment rate"),
  West_Germany = balance_table$tab.pred[, 1],
  Synthetic = balance_table$tab.pred[, 2],
  OECD = balance_table$tab.pred[, 3]
)

# Format the table
knitr::kable(
  balance_df,
  digits = 1,
  col.names = c("Predictor", "West Germany", "Synthetic", "OECD"),
  caption = "Economic Growth Predictor Means Before German Reunification"
)
```

```{r synth-weights-table}
# Report non-zero weights
weights_df <- data.frame(
  Country = dataprep_out$names.and.numbers[-1, 1],
  Weight = as.numeric(synth_out$solution.w)
)

nonzero_weights <- weights_df %>%
  filter(Weight > 0.001) %>%
  arrange(desc(Weight))

knitr::kable(
  nonzero_weights,
  digits = 3,
  col.names = c("Country", "Synthetic Control Weight"),
  caption = "Non-Zero Weights for Synthetic West Germany"
)
```

```{r synth-weights-plot}
# Visualize country weights
lookup <- dataprep_out$names.and.numbers
weights_plot_df <- data.frame(
  Unit = as.numeric(rownames(synth_out$solution.w)),
  Weight = synth_out$solution.w[, 1]
) %>%
  left_join(lookup, by = c("Unit" = "unit.numbers")) %>%
  rename(Country = unit.names) %>%
  filter(Weight > 0.001) %>%
  arrange(desc(Weight))

ggplot(weights_plot_df, aes(x = reorder(Country, Weight), y = Weight)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Synthetic Control Weights",
    x = "Country",
    y = "Weight"
  ) +
  theme_minimal()
```

**Interpretation:**

[Which countries contribute most to synthetic West Germany? Is pre-treatment balance good?]

---

## (d) Effect Size & Permutation Test

### Tasks:

1. Plot actual vs. synthetic GDP per capita trajectory (pre- and post-treatment)
2. Calculate estimated effect (gap) in first few post-treatment years and average post-treatment gap
3. Perform **permutation (placebo) test** by reassigning treatment to each control country
4. Report where treated unit's gap falls in the distribution (approximate p-value)
5. Interpret: What does this suggest about the economic impact of reunification?

```{r synth-trajectory-plot}
# Task (d) 1: Plot actual vs synthetic GDP per capita trajectory
path.plot(
  synth.res = synth_out,
  dataprep.res = dataprep_out,
  Ylab = "Real GDP per capita (1985 USD, thousand)",
  Xlab = "Year",
  Ylim = c(15000, 35000),
  Legend = c("West Germany", "Synthetic West Germany"),
  Legend.position = "topleft",
  Main = "West Germany vs. Synthetic Control: GDP per Capita"
)

# Add vertical line at treatment year
abline(v = 1990, lty = 2, col = "red")
```

```{r synth-gap-calculation}
# Task (d) 2: Calculate estimated effect gaps

# Calculate the raw gaps (Treated - Synthetic)
gaps <- dataprep_out$Y1plot - (dataprep_out$Y0plot %*% synth_out$solution.w)
years <- as.numeric(rownames(gaps))

# First few post-treatment years (1990-1992)
first_years_idx <- years %in% 1990:1992
avg_gap_first_years <- mean(gaps[first_years_idx])

# Average post-treatment gap (1990-2003)
post_years_idx <- years >= 1990
avg_gap_post <- mean(gaps[post_years_idx])

# Print results
cat("Average gap in first post-treatment years (1990-1992):", 
    round(avg_gap_first_years, 2), "USD\n")
cat("Average post-treatment gap (1990-2003):", 
    round(avg_gap_post, 2), "USD\n\n")

# Plot the gap over time
gaps.plot(
  synth.res = synth_out,
  dataprep.res = dataprep_out,
  Ylab = "GDP per capita gap (West Germany - Synthetic)",
  Xlab = "Year",
  Main = "Effect of Reunification on GDP per Capita",
  Ylim = c(-7000,7000)
)
```

```{r synth-permutation-test}
# Tasks (d) 3-4: Perform permutation (placebo) test

# Get list of all control units
control_units <- setdiff(unique(repgermany$index), west_germany_id)

# Store results
placebo_gaps <- matrix(NA, nrow = length(years), ncol = length(control_units))
colnames(placebo_gaps) <- control_units
rownames(placebo_gaps) <- years

# Store RMSPE for each placebo
pre_rmspe_placebo <- numeric(length(control_units))
post_rmspe_placebo <- numeric(length(control_units))

cat("Running permutation test for", length(control_units), "control units...\n")

# Loop through each control unit and run synthetic control
for (i in 1:length(control_units)) {
  
  placebo_unit <- control_units[i]
  
  # Skip if this would cause issues
  tryCatch({
    
    # Suppress output
    invisible(capture.output({
      
      # Prepare data with current control unit as "treated"
      dataprep_placebo <- dataprep(
        foo = as.data.frame(repgermany),
        predictors = c("gdp", "trade", "infrate", "industry"),
        predictors.op = "mean",
        time.predictors.prior = 1981:1990,
        special.predictors = list(
          list("invest80", 1980:1985, "mean"),
          list("schooling", 1980:1985, "mean")
        ),
        dependent = "gdp",
        unit.variable = "index",
        unit.names.variable = "country",
        time.variable = "year",
        treatment.identifier = placebo_unit,
        controls.identifier = setdiff(unique(repgermany$index), placebo_unit),
        time.optimize.ssr = 1960:1989,
        time.plot = 1960:2003
      )
      
      # Run synthetic control
      synth_placebo <- synth(data.prep.obj = dataprep_placebo, method = "BFGS")
      
    }))
    
    # Calculate gaps for this placebo
    placebo_gaps[, i] <- dataprep_placebo$Y1plot - 
      (dataprep_placebo$Y0plot %*% synth_placebo$solution.w)
    
    # Calculate RMSPE
    pre_idx <- years < 1990
    post_idx <- years >= 1990
    pre_rmspe_placebo[i] <- sqrt(mean(placebo_gaps[pre_idx, i]^2))
    post_rmspe_placebo[i] <- sqrt(mean(placebo_gaps[post_idx, i]^2))
    
  }, error = function(e) {
    cat("Error with unit", placebo_unit, "\n")
  })
}

# Calculate RMSPE for actual West Germany
pre_idx <- years < 1990
post_idx <- years >= 1990
pre_rmspe_wg <- sqrt(mean(gaps[pre_idx]^2))
post_rmspe_wg <- sqrt(mean(gaps[post_idx]^2))
rmspe_ratio_wg <- post_rmspe_wg / pre_rmspe_wg

# Calculate RMSPE ratios for placebos
rmspe_ratio_placebo <- post_rmspe_placebo / pre_rmspe_placebo

# Calculate p-value: proportion of placebos with ratio >= West Germany's ratio
p_value <- mean(rmspe_ratio_placebo >= rmspe_ratio_wg, na.rm = TRUE)
rank_wg <- sum(rmspe_ratio_placebo >= rmspe_ratio_wg, na.rm = TRUE) + 1
total_units <- sum(!is.na(rmspe_ratio_placebo)) + 1

# Create a summary data frame
perm_test_results <- data.frame(
  Metric = c("West Germany RMSPE Ratio", "Rank of West Germany", "Total Units", "Approximate p-value"),
  Value = c(round(rmspe_ratio_wg, 3), rank_wg, total_units, round(p_value, 3))
)

perm_test_results %>%
  kable(
    caption = "Permutation (Placebo) Test Results for Synthetic Control Analysis",
    col.names = c("Metric", "Value")
  ) %>%
  kable_styling(
    full_width = FALSE,
    bootstrap_options = c("striped", "hover", "condensed")
  ) 

```

```{r synth-placebo-plots}
# Plot: West Germany gap vs all placebo gaps
gaps.plot(
  synth.res = synth_out,
  dataprep.res = dataprep_out,
  Ylab = "GDP per capita gap",
  Xlab = "Year",
  Main = "West Germany vs. Placebo Tests",
  Ylim = c(-7000, 7000)
)

# Add placebo lines
for (i in 1:ncol(placebo_gaps)) {
  lines(years, placebo_gaps[, i], col = "gray", lwd = 0.5)
}

# Re-add West Germany line on top
lines(years, gaps, col = "black", lwd = 2)
abline(v = 1990, lty = 2, col = "red")
legend("bottomleft", 
       legend = c("West Germany", "Placebo units"),
       col = c("black", "gray"),
       lwd = c(2, 1))

# Alternative plot: RMSPE ratio distribution
hist(rmspe_ratio_placebo, 
     breaks = 20,
     main = "Distribution of Post/Pre RMSPE Ratios",
     xlab = "RMSPE Ratio (Post/Pre)",
     col = "lightblue",
     border = "white")
abline(v = rmspe_ratio_wg, col = "red", lwd = 2, lty = 2)
legend("topright", 
       legend = "West Germany",
       col = "red",
       lwd = 2,
       lty = 2)
```

**Interpretation (Task d.5):**

[What does this suggest about the economic impact of reunification?]

---

## (e) Placebo Test on Earlier Years

### Tasks:

1. Conduct placebo treatment year **before** actual 1990 treatment (e.g., 1975)
2. Re-estimate synthetic control and plot the gap
3. What does pre-treatment gap behavior tell you about parallel-trajectory assumption?
4. Comment on how convincing you find the main causal estimate

```{r synth-placebo-pretreatment}
# Task (e) 1: Conduct placebo test with earlier treatment year

# Define the placebo year
placebo_year <- 1975

# Prepare data for the placebo run
invisible(capture.output({
  dataprep_placebo_1975 <- dataprep(
    foo = as.data.frame(repgermany),
    predictors = c("gdp", "trade", "infrate", "industry"),
    predictors.op = "mean",
    # Use period prior to placebo year for predictors
    time.predictors.prior = 1965:1974, 
    special.predictors = list(
      # Keep original time period for static predictors
      list("invest80", 1980:1985, "mean"), 
      list("schooling", 1980:1985, "mean")
    ),
    dependent = "gdp",
    unit.variable = "index",
    unit.names.variable = "country",
    time.variable = "year",
    treatment.identifier = west_germany_id,
    controls.identifier = setdiff(unique(repgermany$index), west_germany_id),
    # Optimize up to placebo year
    time.optimize.ssr = 1960:(placebo_year - 1),
    time.plot = 1960:2003
  )

  # Re-estimate Synthetic Control
  synth_placebo_1975 <- synth(data.prep.obj = dataprep_placebo_1975, method = "BFGS")
}))
```

```{r synth-placebo-plot}
# Task (e) 2: Plot the gap for the placebo run

# Plot trajectory
path.plot(
  synth.res = synth_placebo_1975,
  dataprep.res = dataprep_placebo_1975,
  Ylab = "Real GDP per capita (1985 USD, thousand)",
  Xlab = "Year",
  Ylim = c(0, 35000),
  Legend = c("West Germany", "Synthetic West Germany (Placebo)"),
  Legend.position = "topleft",
  Main = paste0("Placebo Test: Treatment Year = ", placebo_year)
)
abline(v = placebo_year, lty = 2, col = "red")

# Plot the gap
gaps.plot(
  synth.res = synth_placebo_1975,
  dataprep.res = dataprep_placebo_1975,
  Ylab = "GDP per capita gap (West Germany - Synthetic)",
  Xlab = "Year",
  Main = paste0("Placebo Test Gap: Treatment Year = ", placebo_year),
  Ylim = c(-7000,7000)
)
abline(v = placebo_year, lty = 2, col = "red")
```

```{r synth-placebo-analysis}
# Calculate gaps for placebo test
gaps_placebo_1975 <- dataprep_placebo_1975$Y1plot - 
  (dataprep_placebo_1975$Y0plot %*% synth_placebo_1975$solution.w)
years_placebo <- as.numeric(rownames(gaps_placebo_1975))

# Calculate RMSPE before and after placebo treatment
pre_placebo_idx <- years_placebo < placebo_year
post_placebo_idx <- years_placebo >= placebo_year

pre_rmspe_placebo_1975 <- sqrt(mean(gaps_placebo_1975[pre_placebo_idx]^2))
post_rmspe_placebo_1975 <- sqrt(mean(gaps_placebo_1975[post_placebo_idx]^2))
rmspe_ratio_placebo_1975 <- post_rmspe_placebo_1975 / pre_rmspe_placebo_1975

# Compare to actual treatment (1990)
results_table <- data.frame(
  Scenario = c("Placebo Treatment (1975)", "Actual Treatment (1990)"),
  Pre_RMSPE = c(pre_rmspe_placebo_1975, NA),
  Post_RMSPE = c(post_rmspe_placebo_1975, NA),
  RMSPE_Ratio = c(rmspe_ratio_placebo_1975, rmspe_ratio_wg)
)

# Display nicely
results_table %>%
  mutate(
    Pre_RMSPE = round(Pre_RMSPE, 2),
    Post_RMSPE = round(Post_RMSPE, 2),
    RMSPE_Ratio = round(RMSPE_Ratio, 3)
  ) %>%
  kable(
    caption = "Placebo (1975) vs Actual (1990) RMSPE Comparison",
    col.names = c("Scenario", "Pre RMSPE", "Post RMSPE", "RMSPE Ratio")
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))

```

**Interpretation (Tasks e.3-4):**

[What does pre-treatment gap behavior tell you about parallel-trajectory assumption? How convincing is the main causal estimate?]

---